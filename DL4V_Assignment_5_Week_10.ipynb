{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL4V-Assignment-5-Week-10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemanthkumar17/DL4CV2020/blob/main/DL4V_Assignment_5_Week_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXTOa4agGbjK"
      },
      "source": [
        "#### **Welcome to Assignment 5 on Deep Learning for Computer Vision.**\n",
        "In this assignment you will learn VAE and Normalizing Flow.\n",
        "\n",
        "#### **Instructions**\n",
        "1. Use Python 3.x to run this notebook\n",
        "3. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
        "you should not change anything else code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
        "4. Read documentation of each function carefully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv_XL5DOp35x"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "Given an input image $x$ and parameters of an Encoder and Decoder of a Variational AutoEncoder(VAE) , find the variational Lower bound of $p(x)$, i.e. probability of $x$.\n",
        "\n",
        "Which of the following expressions holds True?\n",
        "\n",
        "1.   $p(x)$ >= 0.8712\n",
        "2.   $p(x)$ >= 0.8998\n",
        "3.   $p(x)$ >= 0.9371\n",
        "4.   $p(x)$ >= 0.9418\n",
        "\n",
        "\n",
        "Hint: \\begin{aligned} \\log p_\\theta(x) &\\geq \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log p_\\theta(x|z) \\right] - \\text{KL}\\left[q_\\phi(z|x) || p(z) \\right] \\end{aligned}\n",
        "Note: Use torch.exp() library function in order to compute exponential.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XByjYrbR_Nu"
      },
      "source": [
        "'''Script to train a Variational AutoEncoder. Note : This Script is given for your reference, Not for\n",
        "   Evaluation Purpose. You are encouraged to modify different part of the code and see how it affects'''\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from os.path import join as oj\n",
        "\n",
        "\n",
        "out_dir = 'samples'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "class Normal(object):\n",
        "    def __init__(self, mu, sigma, log_sigma, v=None, r=None):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma  # either stdev diagonal itself, or stdev diagonal from decomposition\n",
        "        self.logsigma = log_sigma\n",
        "        dim = mu.get_shape()\n",
        "        if v is None:\n",
        "            v = torch.FloatTensor(*dim)\n",
        "        if r is None:\n",
        "            r = torch.FloatTensor(*dim)\n",
        "        self.v = v\n",
        "        self.r = r\n",
        "\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        return F.relu(self.linear2(x))\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        return F.relu(self.linear2(x))\n",
        "\n",
        "\n",
        "class VAE(torch.nn.Module):\n",
        "    latent_dim = 8\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self._enc_mu = torch.nn.Linear(100, 8)\n",
        "        self._enc_log_sigma = torch.nn.Linear(100, 8)\n",
        "\n",
        "    def _sample_latent(self, h_enc):\n",
        "        \"\"\"\n",
        "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
        "        \"\"\"\n",
        "        mu = self._enc_mu(h_enc)\n",
        "        log_sigma = self._enc_log_sigma(h_enc)\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
        "\n",
        "        self.z_mean = mu\n",
        "        self.z_sigma = sigma\n",
        "\n",
        "        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
        "\n",
        "    def forward(self, state):\n",
        "        h_enc = self.encoder(state)\n",
        "        z = self._sample_latent(h_enc)\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "def latent_loss(z_mean, z_stddev):\n",
        "    mean_sq = z_mean * z_mean\n",
        "    stddev_sq = z_stddev * z_stddev\n",
        "    return 0.5 * torch.mean(mean_sq + stddev_sq - torch.log(stddev_sq) - 1)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    input_dim = 28 * 28\n",
        "    batch_size = 32\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor()])\n",
        "    mnist = torchvision.datasets.MNIST('./', download=True, transform=transform)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(mnist, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "\n",
        "    print('Number of samples: ', len(mnist))\n",
        "\n",
        "    encoder = Encoder(input_dim, 100, 100)\n",
        "    decoder = Decoder(8, 100, input_dim)\n",
        "    vae = VAE(encoder, decoder)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    optimizer = optim.Adam(vae.parameters(), lr=0.0001)\n",
        "    l = None\n",
        "    \n",
        "    for epoch in range(25):\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            inputs, classes = data\n",
        "            inputs, classes = Variable(inputs.resize_(batch_size, input_dim)), Variable(classes)\n",
        "            optimizer.zero_grad()\n",
        "            dec = vae(inputs)\n",
        "            ll = latent_loss(vae.z_mean, vae.z_sigma)\n",
        "            loss = criterion(dec, inputs) + ll\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            l = loss.item()\n",
        "        print(epoch, l)\n",
        "    torch.save(vae.state_dict(), oj(out_dir, f'vae_final.pth'))\n",
        "\n",
        "    plt.imshow(vae(inputs).data[0].numpy().reshape(28, 28), cmap='gray')\n",
        "    plt.show(block=True)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP-0rMj6cvWW"
      },
      "source": [
        "# Load a given Pretrained VAE model, and compute lower bound of p(x), given input image x\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from os.path import join as oj\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "out_dir = 'samples'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "class Normal(object):\n",
        "    def __init__(self, mu, sigma, log_sigma, v=None, r=None):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma  # either stdev diagonal itself, or stdev diagonal from decomposition\n",
        "        self.logsigma = log_sigma\n",
        "        dim = mu.get_shape()\n",
        "        if v is None:\n",
        "            v = torch.FloatTensor(*dim)\n",
        "        if r is None:\n",
        "            r = torch.FloatTensor(*dim)\n",
        "        self.v = v\n",
        "        self.r = r\n",
        "\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        return F.relu(self.linear2(x))\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        return F.relu(self.linear2(x))\n",
        "\n",
        "\n",
        "class VAE(torch.nn.Module):\n",
        "    latent_dim = 8\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self._enc_mu = torch.nn.Linear(100, 8)\n",
        "        self._enc_log_sigma = torch.nn.Linear(100, 8)\n",
        "\n",
        "    def _sample_latent(self, h_enc):\n",
        "        \"\"\"\n",
        "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
        "        \"\"\"\n",
        "        mu = self._enc_mu(h_enc)\n",
        "        log_sigma = self._enc_log_sigma(h_enc)\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
        "\n",
        "        self.z_mean = mu\n",
        "        self.z_sigma = sigma\n",
        "\n",
        "        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
        "\n",
        "    def forward(self, state):\n",
        "        h_enc = self.encoder(state)\n",
        "        z = self._sample_latent(h_enc)\n",
        "        return self.decoder(z)\n",
        "\n",
        "\n",
        "def latent_loss(z_mean, z_stddev):\n",
        "    mean_sq = z_mean * z_mean\n",
        "    stddev_sq = z_stddev * z_stddev\n",
        "    return 0.5 * torch.mean(mean_sq + stddev_sq - torch.log(stddev_sq) - 1)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor()])\n",
        "    mnist = torchvision.datasets.MNIST('./', download=True, transform=transform)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(mnist, batch_size=1,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "    for input in dataloader:\n",
        "      im0,_ = input \n",
        "      break\n",
        "    \n",
        "    # Visualize the input image\n",
        "\n",
        "    plt.imshow(im0.data[0].numpy().reshape(28, 28), cmap='gray')\n",
        "    plt.show(block=True)\n",
        "\n",
        "    ## Find out P(im0) >= ? i.e. Lower bound of Probability of im0 using the functions defined above\n",
        "    \n",
        "    ### YOUR CODE STARTS HERE ###\n",
        "\n",
        "    \n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZra1WTzFsOr"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "Compute \"Transformed\" Probability Density Function (pdf) of $\\mathbf{y}$, i.e.  $q_1(\\mathbf{y})$. Where, $\\mathbf{y}$ is obtained from $\\mathbf{z}$ via an invertible transformation $f$ as shown below:\n",
        "\n",
        "Let $\\mathbf{z}\\sim q_0(\\mathbf{z})$ where $q_0(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z};\\mathbf{0},\\mathbf{I})$.  \n",
        "Let $f(\\mathbf{z})$ be an invertible transformation given by \n",
        "$$ \\mathbf{y} = f(\\mathbf{z}) = \\mathbf{z} + \\mathbf{u}h(\\mathbf{w}^\\top\\mathbf{z}+b)$$\n",
        "The pdf of $\\mathbf{y}$ is given by\n",
        "$q_1(\\mathbf{y})$\n",
        "\n",
        "Which of the following are correct about $q_1(\\mathbf{y})$; given the values of $q_0(\\mathbf{z})$ , $\\mathbf{f}$ and all the parameters required to compute $f(\\mathbf{z})$?\n",
        "\n",
        "1.   Mean = 0.0473 ; Std = 0.0636\n",
        "2.   Mean = 0.0581 ; Std = 0.0527\n",
        "3.   Mean = 0.0614 ; Std = 0.0491\n",
        "4.   Mean = 0.0512 ; Std = 0.0561\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Hint: \n",
        "$$q_1(\\mathbf{y}) = q_0(\\mathbf{z})\\left|\\det\\frac{\\partial f}{\\partial \\mathbf{z}}\\right|^{-1}$$\n",
        "\n",
        "$\\left|\\det\\frac{\\partial f}{\\partial \\mathbf{z}}\\right|$ can be computed as follows\n",
        "$$ \\psi(\\mathbf{z}) = h'(\\mathbf{w}^\\top\\mathbf{z}+b)\\mathbf{w} $$\n",
        "$$\\left|\\det\\frac{\\partial f}{\\partial \\mathbf{z}}\\right| = |1 + \\mathbf{u}^\\top\\psi(\\mathbf{z})|$$\n",
        "Here, we set $h(x)=\\tanh(x)$ which gives us $h'(x)=(1-\\tanh^2(x))$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5wRHdtvFynH"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.linalg as LA\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "## Define Parameters of Invertible transformation\n",
        "w = np.array([4., 0])\n",
        "u = np.array([2., 0])\n",
        "b = 0\n",
        "\n",
        "## Define and Instantiate probability Density function q0(z)\n",
        "\n",
        "def mvn_pdf(X, mu=np.array([[0,0]]), sig=np.eye(2)):\n",
        "    sqrt_det_2pi_sig = np.sqrt(2 * np.pi * LA.det(sig))\n",
        "    sig_inv = LA.inv(sig)\n",
        "    X = X[:, None, :] - mu[None, :, :]\n",
        "    return np.exp(-np.matmul(np.matmul(X, np.expand_dims(sig_inv, 0)), (X.transpose(0, 2, 1)))/2)/sqrt_det_2pi_sig\n",
        "\n",
        "r = np.linspace(-3,3,1000)\n",
        "z = np.array(np.meshgrid(r, r)).transpose(1, 2, 0)\n",
        "z = np.reshape(z, [z.shape[0] * z.shape[1], -1])\n",
        "q0 = mvn_pdf(z)\n",
        "\n",
        "\n",
        "### YOUR CODE STATRS HERE ###\n",
        "\n",
        "## compute q1(y) from given q0 and all the parameters of transformation f\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE ###\n",
        "\n",
        "print (\"Mean of q1(y): \",np.mean(q1))\n",
        "print (\"Standard Deviation of q1(y): \",np.std(q1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}